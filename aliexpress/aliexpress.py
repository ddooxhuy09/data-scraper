import sys
from undetected_playwright.async_api import async_playwright
import asyncio
import random
import pandas as pd
import re

async def scrape_page(page, page_num):
    print(f"üîç ƒêang thu th·∫≠p d·ªØ li·ªáu trang {page_num}...")

    for _ in range(10):
        await page.evaluate("window.scrollBy(0, window.innerHeight);")
        await asyncio.sleep(random.uniform(0.5, 1))
    
    items = await page.query_selector_all("div.multi--outWrapper--SeJ8lrF")

    results = []
    for item in items:
        link_element = await item.query_selector("a.multi--container--1UZxxHY")
        title_element = await item.query_selector("h3.multi--titleText--nXeOvyr")
        sold_element = await item.query_selector("span.multi--trade--Ktbl2jB")
        price_element = await item.query_selector("div.multi--price-sale--U-S0jtj")
        
        if link_element and title_element and price_element:
            url = await link_element.get_attribute("href")
            title = await title_element.inner_text()
            sold_text = await sold_element.inner_text() if sold_element else "0"
            price = await price_element.inner_text()
            
            results.append({
                "URL": f"https:{url}",
                "Title": title.strip(),
                "Sold": sold_text.strip(),
                "Price": price.strip(),
            })
    
    return results

async def open_aliexpress_with_keyword(keyword):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context()
        page = await context.new_page()

        all_results = []
        page_num = 1

        while True:
            url = f"https://vi.aliexpress.com/w/wholesale-{keyword}.html?page={page_num}"
            print(f"üõí ƒêang m·ªü trang {page_num}: {url}")
            await page.goto(url)
            await page.wait_for_timeout(5000)  # Ch·ªù trang t·∫£i ƒë·∫ßy ƒë·ªß
            
            # Thu th·∫≠p d·ªØ li·ªáu t·ª´ trang hi·ªán t·∫°i
            results = await scrape_page(page, page_num)

            if not results:
                print(f"‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu tr√™n trang {page_num}, d·ª´ng l·∫°i.")
                break
            
            all_results.extend(results)
            page_num += 1  # Chuy·ªÉn sang trang ti·∫øp theo
        
        await browser.close()
        
        return all_results

async def main():
    keyword = sys.argv[1]
    results = await open_aliexpress_with_keyword(keyword)

    # L∆∞u v√†o CSV
    if results:
        df = pd.DataFrame(results)
        
        # üõ† Tr√≠ch xu·∫•t s·ªë l∆∞·ª£ng b√°n t·ª´ chu·ªói d·∫°ng "1000+ sold"
        df['sold_numeric'] = df['Sold'].apply(lambda x: int(re.search(r'(\d+)', x.replace(",", "")).group(1)) if re.search(r'(\d+)', x.replace(",", "")) else 0)

        # üõ† L·ªçc c√°c s·∫£n ph·∫©m c√≥ s·ªë l∆∞·ª£ng b√°n > 1000
        df_filtered = df[df['sold_numeric'] >= 1000].drop(columns=['sold_numeric'])  # X√≥a c·ªôt t·∫°m th·ªùi

        # üõ† L∆∞u k·∫øt qu·∫£ v√†o CSV
        df_filtered.to_csv(f"{keyword}_aliexpress.csv", index=False, encoding='utf-8')
        print(f"‚úÖ ƒê√£ l∆∞u d·ªØ li·ªáu v√†o {keyword}_aliexpress.csv (ch·ªâ c√°c s·∫£n ph·∫©m c√≥ Sold > 1000)")
    else:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m n√†o c√≥ s·ªë l∆∞·ª£ng b√°n > 1000!")

# Ch·∫°y ch∆∞∆°ng tr√¨nh
asyncio.run(main())
